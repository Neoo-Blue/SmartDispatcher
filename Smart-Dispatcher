"""
title: Smart Dispatcher
author: YourName
version: 4.12
description: Automatically routes and executes tasks with the most appropriate AI model (August 2025) - with failover on quota exhaustion, Gemini preference for simple queries, and expanded categories
"""

import os
import openai
import google.generativeai as genai
import requests
import logging
from pydantic import BaseModel, Field
from typing import Union, Generator, Iterator, List, Dict

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class Pipe:
    class Valves(BaseModel):
        # API Configuration
        openai_api_key: str = Field(
            default="", description="OpenAI API Key for GPT models"
        )
        google_api_key: str = Field(
            default="", description="Google API Key for Gemini models"
        )
        xai_api_key: str = Field(default="", description="xAI API Key for Grok models")
        perplexity_api_key: str = Field(
            default="", description="Perplexity API Key for Sonar models"
        )

        # Classifier Configuration
        classifier_model: str = Field(
            default="gemini-2.5-pro", description="Model used for task classification"
        )

        # Model Management (Updated August 2025)
        allowed_models: str = Field(
            default="models/gemini-2.5-flash,models/gemini-2.5-pro,sonar-deep-research,sonar-reasoning-pro,sonar-pro,sonar-reasoning,sonar,grok-4-0709,grok-4-0709-eu,grok-3,chatgpt-4o-latest,gpt-4o,gpt-4,gpt-4.1,grok-3-fast,gpt-4o-mini,gpt-3.5-turbo,models/aqa,gpt-5,gpt-5-mini,gpt-5-nano",
            description="Comma-separated list of allowed models",
        )
        disabled_models: str = Field(
            default="gpt-5,gpt-5-mini,gpt-5-nano",
            description="Comma-separated list of disabled models",
        )

        # Display Options
        show_selection_info: bool = Field(
            default=True, description="Show model selection information"
        )
        execute_with_selected_model: bool = Field(
            default=True, description="Actually execute with the selected model"
        )
        show_reasoning_details: bool = Field(
            default=True, description="Show detailed reasoning for model selection"
        )

    def __init__(self):
        self.valves = self.Valves()
        self._setup_available_models()

    def _get_api_keys(self) -> Dict[str, str]:
        """Get API keys from multiple sources"""
        logger.info("Fetching API keys")
        api_keys = {
            "openai": self.valves.openai_api_key.strip()
            or os.getenv("OPENAI_API_KEY")
            or "",
            "google": self.valves.google_api_key.strip()
            or os.getenv("GOOGLE_API_KEY")
            or "",
            "xai": self.valves.xai_api_key.strip() or os.getenv("XAI_API_KEY") or "",
            "perplexity": self.valves.perplexity_api_key.strip()
            or os.getenv("PERPLEXITY_API_KEY")
            or "",
        }
        logger.info(
            f"API keys retrieved: { {k: 'set' if v else 'unset' for k, v in api_keys.items()} }"
        )
        return api_keys

    def _get_allowed_models(self) -> Dict[str, Dict]:
        """Get list of allowed models based on configuration"""
        logger.info("Fetching allowed models")
        allowed_list = [
            m.strip() for m in self.valves.allowed_models.split(",") if m.strip()
        ]
        disabled_list = [
            m.strip() for m in self.valves.disabled_models.split(",") if m.strip()
        ]

        allowed_models = {}
        for model_id in allowed_list:
            if model_id in self.all_models and model_id not in disabled_list:
                allowed_models[model_id] = self.all_models[model_id]

        logger.info(f"Allowed models: {list(allowed_models.keys())}")
        return allowed_models

    def _setup_available_models(self):
        """Define available models and their specialties"""
        self.all_models = {
            # OpenAI Models (2025)
            "chatgpt-4o-latest": {
                "type": "openai",
                "specialty": [
                    "multimodal",
                    "vision",
                    "reasoning",
                    "coding",
                    "advanced",
                    "creative_writing",
                    "question_answering",
                ],
                "description": "Latest ChatGPT-4o model with enhanced capabilities",
                "cost_tier": "high",
            },
            "gpt-3.5-turbo": {
                "type": "openai",
                "specialty": [
                    "general",
                    "fast_response",
                    "cost_effective",
                    "question_answering",
                ],
                "description": "Efficient GPT-3.5 Turbo for general-purpose tasks",
                "cost_tier": "low",
            },
            "gpt-4": {
                "type": "openai",
                "specialty": ["reasoning", "coding", "analysis"],
                "description": "Original GPT-4 model with strong reasoning capabilities",
                "cost_tier": "high",
            },
            "gpt-4.1": {
                "type": "openai",
                "specialty": ["reasoning", "coding", "analysis", "improved"],
                "description": "Improved GPT-4 model with enhanced performance",
                "cost_tier": "high",
            },
            "gpt-4o": {
                "type": "openai",
                "specialty": [
                    "multimodal",
                    "vision",
                    "reasoning",
                    "coding",
                    "creative_writing",
                ],
                "description": "Advanced multimodal GPT-4o model",
                "cost_tier": "high",
            },
            "gpt-4o-mini": {
                "type": "openai",
                "specialty": [
                    "general",
                    "fast_response",
                    "cost_effective",
                    "question_answering",
                ],
                "description": "Efficient GPT-4o variant for general use",
                "cost_tier": "medium",
            },
            "gpt-5": {
                "type": "openai",
                "specialty": [
                    "multimodal",
                    "reasoning",
                    "coding",
                    "advanced",
                    "creative_writing",
                    "question_answering",
                ],
                "description": "Hypothetical GPT-5 model with superior capabilities (assumed)",
                "cost_tier": "premium",
            },
            "gpt-5-mini": {
                "type": "openai",
                "specialty": [
                    "general",
                    "fast_response",
                    "cost_effective",
                    "question_answering",
                ],
                "description": "Hypothetical efficient GPT-5 variant (assumed)",
                "cost_tier": "medium",
            },
            "gpt-5-nano": {
                "type": "openai",
                "specialty": [
                    "general",
                    "fast_response",
                    "highly_cost_effective",
                    "question_answering",
                ],
                "description": "Hypothetical ultra-efficient GPT-5 variant (assumed)",
                "cost_tier": "low",
            },
            # xAI Grok Models
            "grok-3": {
                "type": "xai",
                "specialty": [
                    "real_time",
                    "current_events",
                    "reasoning",
                    "advanced",
                    "data_analysis",
                    "technical_analysis",
                ],
                "description": "Hypothetical Grok-3 with enhanced reasoning (assumed)",
                "cost_tier": "high",
            },
            "grok-3-fast": {
                "type": "xai",
                "specialty": ["real_time", "fast_response", "current_events"],
                "description": "Hypothetical fast Grok-3 variant (assumed)",
                "cost_tier": "medium",
            },
            "grok-4-0709": {
                "type": "xai",
                "specialty": [
                    "real_time",
                    "current_events",
                    "reasoning",
                    "multimodal",
                    "data_analysis",
                    "technical_analysis",
                ],
                "description": "Hypothetical Grok-4 model (assumed, July 2025 release)",
                "cost_tier": "premium",
            },
            "grok-4-0709-eu": {
                "type": "xai",
                "specialty": [
                    "real_time",
                    "current_events",
                    "reasoning",
                    "multimodal",
                    "data_analysis",
                    "technical_analysis",
                ],
                "description": "Hypothetical Grok-4 EU variant with region-specific compliance (assumed)",
                "cost_tier": "premium",
            },
            # Google Gemini Models
            "models/gemini-2.5-flash": {
                "type": "google",
                "specialty": [
                    "fast_response",
                    "general",
                    "classification",
                    "question_answering",
                    "simple_queries",
                ],
                "description": "Fast and efficient Gemini 2.5 Flash variant",
                "cost_tier": "low",
            },
            "models/gemini-2.5-pro": {
                "type": "google",
                "specialty": [
                    "long_context",
                    "multimodal",
                    "analysis",
                    "reasoning",
                    "data_analysis",
                    "creative_writing",
                    "question_answering",
                    "simple_queries",
                ],
                "description": "Advanced Gemini 2.5 Pro with long context",
                "cost_tier": "high",
            },
            # Perplexity Sonar Models
            "sonar": {
                "type": "perplexity",
                "specialty": [
                    "search",
                    "current_info",
                    "research",
                    "question_answering",
                ],
                "description": "Standard Sonar model with web search",
                "cost_tier": "low",
            },
            "sonar-deep-research": {
                "type": "perplexity",
                "specialty": [
                    "search",
                    "current_info",
                    "research",
                    "deep_analysis",
                    "technical_analysis",
                ],
                "description": "Hypothetical Sonar model for in-depth research (assumed)",
                "cost_tier": "medium",
            },
            "sonar-pro": {
                "type": "perplexity",
                "specialty": [
                    "search",
                    "current_info",
                    "research",
                    "analysis",
                    "question_answering",
                ],
                "description": "Professional Sonar with enhanced capabilities",
                "cost_tier": "medium",
            },
            "sonar-reasoning": {
                "type": "perplexity",
                "specialty": ["search", "reasoning", "research", "question_answering"],
                "description": "Hypothetical Sonar model with enhanced reasoning (assumed)",
                "cost_tier": "medium",
            },
            "sonar-reasoning-pro": {
                "type": "perplexity",
                "specialty": [
                    "search",
                    "reasoning",
                    "research",
                    "advanced",
                    "technical_analysis",
                ],
                "description": "Hypothetical advanced Sonar reasoning model (assumed)",
                "cost_tier": "high",
            },
            # Other Models
            "models/aqa": {
                "type": "unknown",
                "specialty": ["question_answering", "general"],
                "description": "Hypothetical AQA model for question answering (assumed)",
                "cost_tier": "medium",
            },
        }

    def pipe(self, body: dict) -> Union[str, Generator, Iterator]:
        """Main pipe function that processes user requests"""
        logger.info(f"Processing query: {body.get('messages', [])}")
        messages = body.get("messages", [])
        if not messages:
            return "âŒ No messages provided in the request."

        user_message = ""

        # Get the latest user message
        for msg in reversed(messages):
            if msg.get("role") == "user":
                user_message = msg.get("content", "")
                break

        if not user_message:
            return "âŒ No user message found in the request."

        # Step 1: Select the best model
        selected_model, reasoning = self._classify_and_select_model(
            user_message, messages
        )

        # Step 2: Build response header
        response = ""
        if self.valves.show_selection_info:
            response += f"ðŸŽ¯ **Smart Dispatcher Active**\n"
            response += f"â€¢ **Selected Model**: {selected_model}\n"
            if self.valves.show_reasoning_details:
                response += f"â€¢ **Reasoning**: {reasoning}\n"
                model_info = self.all_models.get(selected_model, {})
                response += (
                    f"â€¢ **Specialties**: {', '.join(model_info.get('specialty', []))}\n"
                )
            response += "\n---\n\n"

        # Step 3: Execute with selected model
        if self.valves.execute_with_selected_model:
            try:
                model_response = self._execute_with_model(
                    selected_model, user_message, messages
                )
                response += model_response
            except Exception as e:
                response += f"âŒ **Error executing with {selected_model}**: {str(e)}"
        else:
            response += f"This task would be processed by **{selected_model}**.\n\n"
            response += f"**Your query**: {user_message}"

        return response

    def _classify_and_select_model(
        self, user_message: str, messages: List[dict]
    ) -> tuple:
        """Use AI classifier to select the best model"""
        allowed_models = self._get_allowed_models()

        if not allowed_models:
            return (
                "models/gemini-2.5-pro",
                "No allowed models configured - using default",
            )

        # Rule-based selection for reliability
        return self._rule_based_selection(user_message, messages, allowed_models)

    def _rule_based_selection(
        self, message: str, messages: List[dict], allowed_models: Dict
    ) -> tuple:
        """Rule-based model selection with refined scenarios and Gemini preference for simple queries"""
        message_lower = message.lower()

        # Simple queries (prefer Gemini Flash/Pro)
        if len(message.split()) <= 3 or any(
            word in message_lower for word in ["hi", "hello", "hey", "greetings"]
        ):
            for model_id in allowed_models:
                if "simple_queries" in allowed_models[model_id]["specialty"]:
                    return (
                        model_id,
                        "Simple query detected - preferring Gemini Flash or Pro model",
                    )

        # Profanity detection - prefer Grok models
        profanity_words = [
            "fuck",
            "shit",
            "damn",
            "ass",
            "bitch",
            "bastard",
            "cunt",
            "dick",
            "piss",
            "cock",
            "hell",
            "crap",
            "bollocks",
            "bugger",
            "bloody",
            "wanker",
        ]
        if any(word in message_lower for word in profanity_words):
            for model_id in allowed_models:
                if allowed_models[model_id]["type"] == "xai":
                    return model_id, "Profanity detected - using Grok model"

        # News/current information queries (prefer Sonar)
        if any(
            word in message_lower
            for word in ["news", "today", "recent", "current", "latest"]
        ):
            for model_id in allowed_models:
                if (
                    "search" in allowed_models[model_id]["specialty"]
                    or "current_info" in allowed_models[model_id]["specialty"]
                ):
                    return (
                        model_id,
                        "News/current information required - using Sonar-capable model",
                    )

        # Search/research queries (prefer Sonar)
        if any(
            word in message_lower for word in ["search", "research", "lookup", "find"]
        ):
            for model_id in allowed_models:
                if (
                    "search" in allowed_models[model_id]["specialty"]
                    or "research" in allowed_models[model_id]["specialty"]
                ):
                    return (
                        model_id,
                        "Search/research required - using Sonar-capable model",
                    )

        # Creative writing tasks (prefer GPT or Gemini)
        if any(
            word in message_lower
            for word in ["write a story", "creative", "fiction", "poem", "script"]
        ):
            for model_id in allowed_models:
                if "creative_writing" in allowed_models[model_id]["specialty"]:
                    return (
                        model_id,
                        "Creative writing task detected - using creative model",
                    )

        # Data analysis tasks (prefer Gemini Pro or Grok)
        if any(
            word in message_lower
            for word in ["data", "dataset", "statistics", "analyze data", "chart"]
        ):
            for model_id in allowed_models:
                if (
                    "data_analysis" in allowed_models[model_id]["specialty"]
                    or "analysis" in allowed_models[model_id]["specialty"]
                ):
                    return (
                        model_id,
                        "Data analysis task detected - using analysis model",
                    )

        # Technical analysis tasks (prefer Grok or Sonar)
        if any(
            word in message_lower
            for word in ["technical", "scientific", "engineering", "tech analysis"]
        ):
            for model_id in allowed_models:
                if "technical_analysis" in allowed_models[model_id]["specialty"]:
                    return (
                        model_id,
                        "Technical analysis task detected - using technical model",
                    )

        # Deep analysis/long context queries (prefer Gemini Pro)
        if any(
            word in message_lower
            for word in ["analyze", "analysis", "deep", "detailed", "long context"]
        ):
            for model_id in allowed_models:
                if (
                    "analysis" in allowed_models[model_id]["specialty"]
                    or "deep_analysis" in allowed_models[model_id]["specialty"]
                    or "long_context" in allowed_models[model_id]["specialty"]
                ):
                    return (
                        model_id,
                        "Deep analysis/long context required - using Gemini/Sonar model",
                    )

        # Classification/general fast queries (prefer Gemini Flash)
        if any(
            word in message_lower
            for word in ["classify", "categorize", "general", "quick"]
        ):
            for model_id in allowed_models:
                if (
                    "classification" in allowed_models[model_id]["specialty"]
                    or "fast_response" in allowed_models[model_id]["specialty"]
                ):
                    return (
                        model_id,
                        "Classification/fast response required - using Gemini Flash model",
                    )

        # Visual/multimodal queries (prefer OpenAI/Grok/Gemini multimodal)
        if any(
            word in message_lower
            for word in ["image", "photo", "picture", "visual", "see"]
        ):
            for model_id in allowed_models:
                if (
                    "vision" in allowed_models[model_id]["specialty"]
                    or "multimodal" in allowed_models[model_id]["specialty"]
                ):
                    return (
                        model_id,
                        "Visual/multimodal task detected - using multimodal model",
                    )

        # Coding queries (prefer OpenAI)
        if any(
            word in message_lower
            for word in ["code", "programming", "python", "javascript", "debug"]
        ):
            for model_id in allowed_models:
                if "coding" in allowed_models[model_id]["specialty"]:
                    return (
                        model_id,
                        "Programming/coding task detected - using coding-capable model",
                    )

        # Math/reasoning queries (prefer OpenAI/Gemini/Grok reasoning)
        if any(
            word in message_lower
            for word in ["math", "calculate", "solve", "reasoning"]
        ):
            for model_id in allowed_models:
                if "reasoning" in allowed_models[model_id]["specialty"]:
                    return (
                        model_id,
                        "Complex reasoning/math task detected - using reasoning model",
                    )

        # Question answering/general queries (prefer Sonar or Gemini)
        if any(
            word in message_lower
            for word in ["what", "who", "where", "when", "why", "how"]
        ):
            for model_id in allowed_models:
                if "question_answering" in allowed_models[model_id]["specialty"]:
                    return (
                        model_id,
                        "Question answering task detected - using Q&A model",
                    )

        # Default to bigger models (advanced, premium/high cost tier)
        for model_id in allowed_models:
            if "advanced" in allowed_models[model_id]["specialty"] or allowed_models[
                model_id
            ]["cost_tier"] in ["premium", "high"]:
                return (
                    model_id,
                    "General query with no clear difference - preferring advanced/premium model",
                )

        # Fallback to Gemini fast response model
        for model_id in allowed_models:
            if (
                "fast_response" in allowed_models[model_id]["specialty"]
                and allowed_models[model_id]["type"] == "google"
            ):
                return (
                    model_id,
                    "General query with no clear difference - preferring Gemini fast response model",
                )

        # Final fallback
        first_model = (
            list(allowed_models.keys())[0]
            if allowed_models
            else "models/gemini-2.5-pro"
        )
        return first_model, "Default selection from available models"

    def _execute_with_model(
        self, model_id: str, user_message: str, messages: List[dict]
    ) -> str:
        """Execute the task with the selected model, with failover only on quota exhaustion"""
        logger.info(f"Attempting execution with model: {model_id}")
        allowed_models = self._get_allowed_models()
        model_config = self.all_models.get(model_id, {})
        model_type = model_config.get("type", "unknown")

        api_keys = self._get_api_keys()

        try:
            if model_type == "openai":
                return self._execute_openai_model(
                    model_id, messages, api_keys["openai"]
                )
            elif model_type == "google":
                return self._execute_google_model(
                    model_id, user_message, messages, api_keys["google"]
                )
            elif model_type == "xai":
                return self._execute_xai_model(model_id, messages, api_keys["xai"])
            elif model_type == "perplexity":
                return self._execute_perplexity_model(
                    model_id, messages, api_keys["perplexity"]
                )
            else:
                raise ValueError(f"Model type '{model_type}' not supported.")
        except Exception as e:
            # Failover only for quota exhaustion errors
            error_msg = str(e).lower()
            is_quota_error = any(
                keyword in error_msg
                for keyword in ["quota", "rate limit", "too many requests", "429"]
            )
            if is_quota_error:
                logger.warning(f"Quota exhaustion for {model_id}: {error_msg}")
                similar_models = [
                    m
                    for m, conf in allowed_models.items()
                    if m != model_id
                    and set(conf.get("specialty", []))
                    & set(model_config.get("specialty", []))
                    and conf.get("type") != model_type
                ]
                if similar_models:
                    failover_model = similar_models[0]
                    logger.info(f"Failing over to {failover_model}")
                    return (
                        f"âš ï¸ Failover from {model_id} due to quota exhaustion: {str(e)}\n\n"
                        + self._execute_with_model(
                            failover_model, user_message, messages
                        )
                    )
                else:
                    logger.error(f"No failover model available for {model_id}")
                    return f"âŒ No failover model available for {model_id}: {str(e)}"
            else:
                logger.error(f"Non-quota error with {model_id}: {str(e)}")
                return f"âŒ Error with {model_id}: {str(e)}"

    def _execute_openai_model(
        self, model_id: str, messages: List[dict], api_key: str
    ) -> str:
        """Execute task using OpenAI models"""
        if not api_key:
            raise ValueError("OpenAI API key not configured.")

        try:
            client = openai.OpenAI(api_key=api_key)

            openai_messages = [
                {
                    "role": msg.get("role", "user"),
                    "content": str(msg.get("content", "")),
                }
                for msg in messages
                if msg.get("content")
            ]

            if not openai_messages:
                raise ValueError("No valid messages found to process.")

            # Use max_completion_tokens and omit temperature for GPT-5 series
            params = {"model": model_id, "messages": openai_messages}
            if model_id in ["gpt-5", "gpt-5-mini", "gpt-5-nano"]:
                params["max_completion_tokens"] = 2000
                # Omit temperature to use default (1) for GPT-5 series
            else:
                params["max_tokens"] = 2000
                params["temperature"] = 0.7

            completion = client.chat.completions.create(**params)

            if completion.choices and len(completion.choices) > 0:
                content = completion.choices[0].message.content
                if content:
                    return content
                else:
                    logger.error(f"Empty content in OpenAI response for {model_id}")
                    raise ValueError(f"Empty content in OpenAI response for {model_id}")
            else:
                logger.error(f"No choices in OpenAI response for {model_id}")
                raise ValueError(f"No response choices returned from OpenAI API")
        except openai.OpenAIError as e:
            error_msg = str(e).lower()
            if "unsupported value" in error_msg and "temperature" in error_msg:
                logger.info(f"Retrying {model_id} without temperature parameter")
                params.pop("temperature", None)
                completion = client.chat.completions.create(**params)
                if completion.choices and len(completion.choices) > 0:
                    content = completion.choices[0].message.content
                    if content:
                        return content
                    else:
                        logger.error(
                            f"Empty content in OpenAI retry response for {model_id}"
                        )
                        raise ValueError(
                            f"Empty content in OpenAI retry response for {model_id}"
                        )
                else:
                    raise ValueError("No response choices returned from OpenAI API")
            elif "unsupported parameter" in error_msg and "max_tokens" in error_msg:
                logger.info(f"Retrying {model_id} with max_completion_tokens")
                params["max_completion_tokens"] = 2000
                params.pop("max_tokens", None)
                completion = client.chat.completions.create(**params)
                if completion.choices and len(completion.choices) > 0:
                    content = completion.choices[0].message.content
                    if content:
                        return content
                    else:
                        logger.error(
                            f"Empty content in OpenAI retry response for {model_id}"
                        )
                        raise ValueError(
                            f"Empty content in OpenAI retry response for {model_id}"
                        )
                else:
                    raise ValueError("No response choices returned from OpenAI API")
            elif "empty content" in error_msg:
                logger.info(f"Retrying {model_id} with reduced tokens")
                params["max_completion_tokens"] = (
                    1000
                    if model_id in ["gpt-5", "gpt-5-mini", "gpt-5-nano"]
                    else params.get("max_tokens", 2000)
                )
                params["max_tokens"] = (
                    1000 if "max_tokens" in params else params.get("max_tokens")
                )
                completion = client.chat.completions.create(**params)
                if completion.choices and len(completion.choices) > 0:
                    content = completion.choices[0].message.content
                    if content:
                        return content
                    else:
                        logger.error(
                            f"Empty content in OpenAI retry response for {model_id}"
                        )
                        raise ValueError(
                            f"Empty content in OpenAI retry response for {model_id}"
                        )
                else:
                    raise ValueError("No response choices returned from OpenAI API")
            elif "quota" in error_msg or "429" in error_msg:
                raise ValueError(f"Quota exceeded for OpenAI: {str(e)}")
            raise ValueError(f"OpenAI API error: {str(e)}")

    def _execute_google_model(
        self, model_id: str, user_message: str, messages: List[dict], api_key: str
    ) -> str:
        """Execute task using Google Gemini models"""
        if not api_key:
            raise ValueError("Google API key not configured.")

        try:
            genai.configure(api_key=api_key)
            model_name = model_id.replace("models/", "")
            model = genai.GenerativeModel(model_name)

            conversation_context = ""
            for msg in messages[:-1]:
                if msg.get("content"):
                    role = "Human" if msg.get("role") == "user" else "Assistant"
                    conversation_context += f"{role}: {msg.get('content', '')}\n"

            prompt = (
                f"{conversation_context}Human: {user_message}"
                if conversation_context
                else user_message
            )

            response = model.generate_content(prompt)
            if hasattr(response, "text") and response.text:
                return response.text
            else:
                logger.error(f"Empty content in Google response for {model_id}")
                raise ValueError(f"Empty content in Google response for {model_id}")
        except Exception as e:
            if "quota" in str(e).lower() or "429" in str(e):
                raise ValueError(f"Quota exceeded for Google: {str(e)}")
            raise ValueError(f"Google API error: {str(e)}")

    def _execute_xai_model(
        self, model_id: str, messages: List[dict], api_key: str
    ) -> str:
        """Execute task using xAI models"""
        if not api_key:
            raise ValueError("xAI API key not configured.")

        try:
            headers = {
                "Authorization": f"Bearer {api_key}",
                "Content-Type": "application/json",
            }

            xai_messages = [
                {"role": msg.get("role", "user"), "content": msg.get("content", "")}
                for msg in messages
                if msg.get("content")
            ]

            data = {
                "messages": xai_messages,
                "model": model_id,
                "temperature": 0.7,
                "max_tokens": 2000,
            }

            response = requests.post(
                "https://api.x.ai/v1/chat/completions",
                headers=headers,
                json=data,
                timeout=60,
            )
            response.raise_for_status()
            result = response.json()

            if "choices" in result and len(result["choices"]) > 0:
                content = result["choices"][0]["message"]["content"]
                if content:
                    return content
                else:
                    logger.error(f"Empty content in xAI response for {model_id}")
                    raise ValueError(f"Empty content in xAI response for {model_id}")
            else:
                logger.error(f"No choices in xAI response for {model_id}")
                raise ValueError(f"No choices in xAI response for {model_id}")
        except requests.exceptions.HTTPError as e:
            if e.response.status_code == 429:
                raise ValueError(f"Quota exceeded for xAI: {str(e)}")
            raise ValueError(f"xAI API error: HTTP {e.response.status_code}")
        except Exception as e:
            raise ValueError(f"xAI API error: {str(e)}")

    def _execute_perplexity_model(
        self, model_id: str, messages: List[dict], api_key: str
    ) -> str:
        """Execute task using Perplexity models"""
        if not api_key:
            raise ValueError("Perplexity API key not configured.")

        try:
            headers = {
                "Authorization": f"Bearer {api_key}",
                "Content-Type": "application/json",
            }

            perplexity_messages = [
                {"role": msg.get("role", "user"), "content": msg.get("content", "")}
                for msg in messages
                if msg.get("content")
            ]

            data = {
                "model": model_id,
                "messages": perplexity_messages,
                "temperature": 0.7,
                "max_tokens": 2000,
            }

            response = requests.post(
                "https://api.perplexity.ai/chat/completions",
                headers=headers,
                json=data,
                timeout=60,
            )
            response.raise_for_status()
            result = response.json()

            if "choices" in result and len(result["choices"]) > 0:
                content = result["choices"][0]["message"]["content"]
                if content:
                    return content
                else:
                    logger.error(f"Empty content in Perplexity response for {model_id}")
                    raise ValueError(
                        f"Empty content in Perplexity response for {model_id}"
                    )
            else:
                logger.error(f"No choices in Perplexity response for {model_id}")
                raise ValueError(f"No choices in Perplexity response for {model_id}")
        except requests.exceptions.HTTPError as e:
            if e.response.status_code == 429:
                raise ValueError(f"Quota exceeded for Perplexity: {str(e)}")
            raise ValueError(f"Perplexity API error: HTTP {e.response.status_code}")
        except Exception as e:
            raise ValueError(f"Perplexity API error: {str(e)}")
